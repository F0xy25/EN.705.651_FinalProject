Prompt Topic:  Activate Interpretable Reason-Based Knowledge

Inspiration:  Emergence of unrequested 'reasoning'.  The LLM added announcements to its action path without being
referenced that tool path in an example, explicitly.  It inferred the importance of the purpose, itself.

Can we discern and evaluate 'reasoning' in the process of multi-agent systems?

Approach:  Request explainability as part of the output.
Response Data:  Includes fields which take freeform text (announcement and event_bot_action) which the model can select
based on 'reasoning'.

Variables:  Predefined reasoning criteria

Aspects for evaluation, injected into prompts as a reasoning path logic task:

- Emotional Relevance (LOW/MED/HIGH)
- Time Sequencing
- Use of Wild Card Variables
- Requested Unique nuances for reasoning
  - What isn't already covered by the logic of this reasoning process described which could be improved by the update?

Prompts:  Include new output variables
-- future relevance
-- emotional importance of the task
-- sequencing importance of the task
-- effects on state
-- unique reasoning nuances

Evaluation Range
-- Numerical examples for state updates
-- Contextual examples for state updates
-- Multi-variables adaptations needed
-- Single-variable adaptations needed
